---
title: GFS
date: 2022-06-28T23:43:28+08:00
lastmod: 2022-06-28T23:43:28+08:00

cover: http://oss.surfaroundtheworld.top/blog-pictures/6_15/whale.jpg

categories:
  - 分布式
tags:
  - GFS
  - MIT6.824
# nolastmod: true
draft: false
---

GFS（Google File System)论文阅读

<!--more-->

基于GFS论文原文，并结合MIT6.824课上的介绍和其他博客的内容，复习并整理一下GFS的内容。这篇论文非常详尽地介绍了GFS的各个方面，这里只总结论文中较为核心部分设计思想。

# 一、GFS是什么

Google 文件系统（Google File System，缩写为 GFS 或 GoogleFS），一种专有分布式文件系统，由 Google 公司开发，运行于 Linux 平台上。尽管 Google 在 2003 年公布了该系统的一些技术细节，但 Google 并没有将该系统的软件部分作为开源软件发布。

GFS 这个创时代的分布式文件系统并将该成果发表在了 2003 年的 SOSP 会议上，之后随着 MapReduce 和 BigTable 论文的发表，Google 的三架马车整整齐齐，掀开了大数据时代的帷幕，引领工业界开始了辉煌的 NoSQL 运动。

# 二、GFS设计的目的

在GFS之前，存在非常多的单机文件系统，但是单机文件系统的容量有限，随着数据量的增大，单机的性能逐渐达到了其上限。

GFS就是为了实现高可用、可拓展、高性能、低成本、可容错的大规模分布式文件系统，同时为用户提供简单的接口而屏蔽了底层的复杂实现细节。

# 三、设计背景

1、组件失效是常态

2、文件存储量非常巨大

3、绝大部分的修改是在文件尾部追加数据，而不是覆盖原有数据

4、协同设计应用程序和文件系统API

# 四、设计预期

1、系统的工作负载主要由两种读操作组成：**大规模的流式读取和小规模的随机读取**。大规模的流式读取通 常一次读取数百 KB 的数据，更常见的是一次读取 1MB 甚至更多的数据。来自同一个客户机的连续操作通常 是读取同一个文件中连续的一个区域。小规模的随机读取通常是在文件某个随机的位置读取几个 KB 数据。 如果应用程序对性能非常关注，**通常的做法是把小规模的随机读取操作合并并排序，之后按顺序批量读取， 这样就避免了在文件中前后来回的移动读取位置。**

2、**系统的工作负载还包括许多大规模的、顺序的、数据追加方式的写操作**。一般情况下，每次写入的数据的大小和大规模读类似。数据一旦被写入后，文件就很少会被修改了。系统支持小规模的随机位置写入操作， 但是可能效率不彰。

3、系统需要支持并发写，即支持数百台机器并发地追加数据到一个文件。操作的原子性和同步开销是主要指标。

4、高性能的稳定网络带宽远比低延迟重要。

# 五、架构

一个 GFS cluster（集群）分为两个组件：

- **单个** master 节点；
- **多个** chunkserver 节点；

一个 GFS 集群同时可以被多个 client（客户）节点访问。

一个 GFS 集群的架构可以用下图表示：

![](http://oss.surfaroundtheworld.top/blog-pictures/6_28/gfs_fig_1_architecture.png)

## Master节点

Master 服务器存储 3 种主要类型的元数据，包括：文件和 Chunk 的命名空间、文件和 Chunk 的对应关系、 每个 Chunk 副本的存放地点。

设计采用的是单一的master节点，其中包含：

1、file name -> array of chunk handles (nv)

2、handles -> list of chunk server (v)

​					  -> version (nv)

​					  -> primary (v)

​					   -> lease expiration (v)

3、LOG（追加写）

4、checkpoint (disk)

## 读操作

在master的基础上，可以实现读操作：

1、客户端把文件名和程序指定的字节偏移，根据固定 的 Chunk 大小，转换成文件的 Chunk 索引。然后，它把文件名和 Chunk 索引发送给 Master 节点。

2、Master 节 点将相应的 Chunk 标识和副本的位置信息发还给客户端。客户端用文件名和 Chunk 索引作为 key 缓存这些信 息。

3、之后客户端发送请求到其中的一个副本处，**一般会选择最近的**。请求信息包含了 Chunk 的标识和字节范 围。在对这个 Chunk 的后续读取操作中，客户端不必再和 Master 节点通讯了，除非缓存的元数据信息过期或 者文件被重新打开。实际上，**客户端通常会在一次请求中查询多个 Chunk 信息，Master 节点的回应也可能包含了紧跟着这些被请求的 Chunk 后面的 Chunk 的信息。**在实际应用中，这些额外的信息在没有任何代价的情 况下，避免了客户端和 Master 节点未来可能会发生的几次通讯。

## Chunk尺寸

GFS中每个chunk的大小选择了64MB，这个尺寸远远大于一般文件系统的 Block size。 每个 Chunk 的副本都以普通 Linux 文件的形式保存在 Chunk 服务器上。

论文中对选择大的chunk尺寸做出以下解释：

1、**减少了客户端和 Master 节点通讯的需求**，因为只 需要一次和 Mater 节点的通信就可以获取 Chunk 的位置信息，之后就可以对同一个 Chunk 进行多次的读写操作。

2、采用较大的 Chunk 尺寸，客户端能够对一个块进行多次操作，这样就可以 通过与 Chunk 服务器保持较长时间的 TCP 连接来减少网络负载

3、选用较大的 Chunk 尺寸减少了 Master 节点需要保存的元数据的数量。这就允许我们把元数据全部放在内存中。

其缺点在于：

小文件包含较少的 Chunk，甚至只有一个 Chunk。当有许多的客户端对同一个小文件进行多次的访问时，存储这些 Chunk 的 Chunk 服务器就会变成热点。

论文中通过使用更大的复制参数来保存可执行文件，以及错开批处理队列系统程序的启动时间的方法解决了这个问题。一个可能的长效解决方案是， 在这种的情况下，允许客户端从其它客户端读取数据。

## 操作日志

**Operation Log** 包含关键 metadata 数据更改的历史记录。它是 GFS 系统的核心。它不仅是元数据的唯一持久记录，而且还充当定义并发操作顺序的逻辑时间线。文件和块，以及它们的版本，都是唯一的，永远由它们创建的逻辑时间来标识。

由于 **Operation Log** 日志非常重要，所以我们必须可靠地存储它，并且在元数据的更改被持久化之前，不能使更改对客户端可见。否则，即使块本身存活下来，我们也会丢失整个文件系统或最近的客户端操作。因此，我们将它复制到多个远程机器上，只有在**本地和远程**将相应的日志记录刷新到磁盘之后才响应客户机操作。在刷新之前，Master 批处理多个日志记录，从而减少刷新和复制对总体系统吞吐量的影响。

Master 通过重播操作日志恢复其文件系统状态。为了最小化启动时间，我们必须保持日志较小。每当日志增长超过一定的大小时，Master 检查其状态，以便通过从本地磁盘加载最新的 checkpoint 并在此之后仅重放有限数量的日志记录来恢复。**checkpoint 是一种紧凑的类似 b 树的形式，可以直接映射到内存中并用于名称空间查找，而无需进行额外的解析。这进一步加快了恢复并提高了可用性**。

Master 恢复仅仅需要最新的完整检查点（checkpoint)和后续日志文件。旧的检查点和日志文件可以自由删除，但通常还是会保留了一些近期的日志，以防止意外。

## 写操作

在实际应用过程中，系统采用的都是追加写而非覆盖写，这样可以并发操作而避免冲突，论文中给出了以下步骤图：

![](http://oss.surfaroundtheworld.top/blog-pictures/6_28/gfs_fig_2.jpeg)

1、客户机向 Master 节点询问哪一个 Chunk 服务器持有当前的租约，即Primary节点，以及其它副本的位置。如果没有一个 Chunk 持有租约，Master 节点就选择其中一个副本建立一个租约（这个步骤在图上没有显示）。

2、Master 节点将主 Chunk 的标识符以及其它副本（又称为 secondary 副本、二级副本）的位置返回给客户 机。客户机缓存这些数据以便后续的操作。只有在主 Chunk 不可用，或者主 Chunk 回复信息表明它已不再持 有租约的时候，客户机才需要重新跟 Master 节点联系。

3、客户端向所有的 replicas 都推送数据，注意此时客户端可以依靠任意顺序进行推送数据，并没有要求此时必须先给 primary 推送数据。所有的 chunkserver(replicas) 都会将推送来的数据存放在内置的 LRU buffer cacahe，缓存中的数据直到被使用或者超时才会被释放。

4、只要 replicas 回复已经接收到了所有数据，那么 Client 就会发送一个 write 指令给 primary 节点，primary 节点为多个写操作计划执行的序号（写操作可能来自于多个 Client），然后将此顺序应用于其本地 I/O 写操作。

5、primary 节点将写操作请求转发给其他两个 replica，它们都将按照 primary 的顺序执行本地的 I/O 写操作；

6、secondaries 从节点返回写成功的响应给 primary 节点；

7、Primary 响应客户端，并返回该过程中发生的错误。注意，这里的错误不仅仅是 Primary 节点的写操作错误，还包括其他两个 replica 节点的写操作错误。如果 primary 自身发生错误，其就不会向其他两个 replica 节点进行转发。另一方面，如果 Client 收到写失败响应，那么其会重新进行写操作尝试，即重新开始 3-7 步。

## 租约机制

在写操作的过程中，为了减少master的管理负担，会有一个副本作为Primary用来同步其他的副本，Primary副本同时会有租约时间，一般为60s。

在master与Primary无法通信的时候，master不会重新选择Primary，而是会等租约到期再重新选择，因为可能只是master与Primary连接出现问题，而Primary还在正常工作，如果选择另一个Primary就会产生Split Brain问题。这也是租约机制的设计的目的之一。